#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Confluence AEGIS Space Sync Tool
AEGIS ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œì»¬ì— ë™ê¸°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python sync_confluence.py --fetch          # ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    python sync_confluence.py --sync           # ì „ì²´ ë™ê¸°í™”
    python sync_confluence.py --search "í‚¤ì›Œë“œ" # ë¬¸ì„œ ê²€ìƒ‰
"""

import os
import sys
import json
import argparse
import requests
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict
import base64

# Windows ì½˜ì†” UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_PATH = Path(__file__).parent / "confluence_config.json"
CACHE_DIR = Path(__file__).parent / "cache"
INDEX_FILE = CACHE_DIR / "page_index.json"


def load_config() -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)


def get_auth_headers() -> dict:
    """
    ì¸ì¦ í—¤ë” ìƒì„±
    í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¸ì¦ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:
    - CONFLUENCE_EMAIL: Atlassian ê³„ì • ì´ë©”ì¼
    - CONFLUENCE_API_TOKEN: API í† í° (https://id.atlassian.com/manage-profile/security/api-tokens)
    """
    email = os.environ.get('CONFLUENCE_EMAIL')
    api_token = os.environ.get('CONFLUENCE_API_TOKEN')
    
    if not email or not api_token:
        raise ValueError(
            "í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:\n"
            "  $env:CONFLUENCE_EMAIL = 'your-email@krafton.com'\n"
            "  $env:CONFLUENCE_API_TOKEN = 'your-api-token'\n\n"
            "API í† í° ë°œê¸‰: https://id.atlassian.com/manage-profile/security/api-tokens"
        )
    
    credentials = base64.b64encode(f"{email}:{api_token}".encode()).decode()
    return {
        "Authorization": f"Basic {credentials}",
        "Content-Type": "application/json",
        "Accept": "application/json"
    }


class ConfluenceSync:
    def __init__(self):
        self.config = load_config()
        self.base_url = self.config['confluence']['base_url']
        self.space_key = self.config['confluence']['space_key']
        self.headers = get_auth_headers()
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    def get_all_pages(self, limit: int = 100) -> List[Dict]:
        """AEGIS ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (REST API v1 ì‚¬ìš©)"""
        # REST API v1 ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
        url = f"{self.base_url}/wiki/rest/api/content"
        params = {
            "spaceKey": self.space_key,
            "type": "page",
            "limit": limit,
            "expand": "version,body.storage,history.createdBy,history.lastUpdated.by"
        }
        
        all_pages = []
        start = 0
        
        while True:
            params["start"] = start
            response = requests.get(url, headers=self.headers, params=params)
            response.raise_for_status()
            data = response.json()
            
            results = data.get('results', [])
            all_pages.extend(results)
            
            # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            if len(results) < limit:
                break
            
            start += limit
        
        return all_pages
    
    def get_page_content(self, page_id: str) -> Dict:
        """íŠ¹ì • í˜ì´ì§€ì˜ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (REST API v1)"""
        url = f"{self.base_url}/wiki/rest/api/content/{page_id}"
        params = {
            "expand": "body.storage,version"
        }
        
        response = requests.get(url, headers=self.headers, params=params)
        response.raise_for_status()
        return response.json()
    
    def search_pages(self, query: str) -> List[Dict]:
        """CQLë¡œ í˜ì´ì§€ ê²€ìƒ‰"""
        url = f"{self.base_url}/wiki/rest/api/content/search"
        params = {
            "cql": f'space="{self.space_key}" AND text~"{query}"',
            "limit": 50
        }
        
        response = requests.get(url, headers=self.headers, params=params)
        response.raise_for_status()
        return response.json().get('results', [])
    
    def sync_all_pages(self) -> dict:
        """ëª¨ë“  í˜ì´ì§€ë¥¼ ë¡œì»¬ì— ë™ê¸°í™”"""
        print(f"ğŸ“¥ AEGIS ìŠ¤í˜ì´ìŠ¤ ë™ê¸°í™” ì‹œì‘...")
        
        pages = self.get_all_pages()
        print(f"ğŸ“„ {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬")
        
        index = {
            "space_key": self.space_key,
            "synced_at": datetime.now().isoformat(),
            "total_pages": len(pages),
            "pages": []
        }
        
        for i, page in enumerate(pages):
            page_id = page['id']
            title = page['title']
            
            print(f"  [{i+1}/{len(pages)}] {title}")
            
            try:
                # ë³¸ë¬¸ ì¶”ì¶œ (ì´ë¯¸ expandë¡œ ê°€ì ¸ì˜´)
                body = page.get('body', {}).get('storage', {}).get('value', '')
                version_info = page.get('version', {})
                history_info = page.get('history', {})
                
                # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ
                created_by = history_info.get('createdBy', {})
                created_by_name = created_by.get('displayName', 'Unknown')
                created_by_email = created_by.get('email', '')
                created_date = history_info.get('createdDate', 'Unknown')
                
                # ìµœì¢… ìˆ˜ì •ì ì •ë³´ ì¶”ì¶œ
                last_updated = history_info.get('lastUpdated', {})
                updated_by = last_updated.get('by', {})
                updated_by_name = updated_by.get('displayName', 'Unknown')
                
                # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
                safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_', 'ê°€-í£')).strip()
                safe_title = safe_title[:50] if len(safe_title) > 50 else safe_title
                filename = f"{page_id}_{safe_title}.md"
                filepath = CACHE_DIR / filename
                
                # í˜ì´ì§€ URL ìƒì„±
                page_url = f"{self.base_url}/wiki/spaces/{self.space_key}/pages/{page_id}"
                
                # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
                md_content = f"""# {title}

> **Page ID**: {page_id}
> **URL**: {page_url}
> **Created By**: {created_by_name}
> **Created Date**: {created_date}
> **Last Updated By**: {updated_by_name}
> **Last Updated**: {version_info.get('when', 'Unknown')}
> **Version**: {version_info.get('number', 'Unknown')}

---

{self._html_to_text(body)}
"""
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                
                index['pages'].append({
                    "id": page_id,
                    "title": title,
                    "filename": filename,
                    "url": page_url,
                    "created_by": created_by_name,
                    "created_by_email": created_by_email,
                    "created_date": created_date,
                    "updated_by": updated_by_name,
                    "updated_date": version_info.get('when', '')
                })
                
            except Exception as e:
                print(f"    âš ï¸ ì˜¤ë¥˜: {e}")
        
        # ì¸ë±ìŠ¤ íŒŒì¼ ì €ì¥
        with open(INDEX_FILE, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ë™ê¸°í™” ì™„ë£Œ! {len(index['pages'])}ê°œ í˜ì´ì§€ ì €ì¥ë¨")
        print(f"ğŸ“ ìºì‹œ ìœ„ì¹˜: {CACHE_DIR}")
        
        return index
    
    def _html_to_text(self, html: str) -> str:
        """ê°„ë‹¨í•œ HTML to Text ë³€í™˜"""
        import re
        
        if not html:
            return ""
        
        # ê¸°ë³¸ì ì¸ HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<br\s*/?>', '\n', html)
        text = re.sub(r'<p[^>]*>', '\n', text)
        text = re.sub(r'</p>', '\n', text)
        text = re.sub(r'<h1[^>]*>(.*?)</h1>', r'\n# \1\n', text)
        text = re.sub(r'<h2[^>]*>(.*?)</h2>', r'\n## \1\n', text)
        text = re.sub(r'<h3[^>]*>(.*?)</h3>', r'\n### \1\n', text)
        text = re.sub(r'<h([4-6])[^>]*>(.*?)</h\1>', r'\n#### \2\n', text)
        text = re.sub(r'<li[^>]*>', '- ', text)
        text = re.sub(r'</li>', '\n', text)
        text = re.sub(r'<strong[^>]*>(.*?)</strong>', r'**\1**', text)
        text = re.sub(r'<em[^>]*>(.*?)</em>', r'*\1*', text)
        text = re.sub(r'<code[^>]*>(.*?)</code>', r'`\1`', text)
        text = re.sub(r'<a[^>]*href="([^"]*)"[^>]*>(.*?)</a>', r'[\2](\1)', text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë³€í™˜
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&amp;', '&')
        text = text.replace('&quot;', '"')
        text = text.replace('&#39;', "'")
        
        # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def get_cached_index(self) -> Optional[dict]:
        """ìºì‹œëœ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        if INDEX_FILE.exists():
            with open(INDEX_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def list_cached_pages(self) -> List[Dict]:
        """ìºì‹œëœ í˜ì´ì§€ ëª©ë¡ ì¶œë ¥"""
        index = self.get_cached_index()
        if not index:
            print("âŒ ìºì‹œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. --syncë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return []
        
        print(f"\nğŸ“š AEGIS ìŠ¤í˜ì´ìŠ¤ ë¬¸ì„œ ëª©ë¡")
        print(f"   ë™ê¸°í™” ì‹œê°„: {index['synced_at']}")
        print(f"   ì´ {index['total_pages']}ê°œ ë¬¸ì„œ\n")
        
        for i, page in enumerate(index['pages'], 1):
            print(f"  {i}. {page['title']}")
            print(f"     â””â”€ {page['url']}")
        
        return index['pages']


def main():
    parser = argparse.ArgumentParser(description='Confluence AEGIS Space Sync Tool')
    parser.add_argument('--fetch', action='store_true', help='í˜ì´ì§€ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸°')
    parser.add_argument('--sync', action='store_true', help='ì „ì²´ ë™ê¸°í™”')
    parser.add_argument('--list', action='store_true', help='ìºì‹œëœ í˜ì´ì§€ ëª©ë¡ ë³´ê¸°')
    parser.add_argument('--search', type=str, help='ë¬¸ì„œ ê²€ìƒ‰')
    
    args = parser.parse_args()
    
    try:
        sync = ConfluenceSync()
        
        if args.fetch:
            pages = sync.get_all_pages()
            print(f"\nğŸ“„ AEGIS ìŠ¤í˜ì´ìŠ¤ì— {len(pages)}ê°œ í˜ì´ì§€ê°€ ìˆìŠµë‹ˆë‹¤:\n")
            for page in pages:
                print(f"  - {page['title']} (ID: {page['id']})")
        
        elif args.sync:
            sync.sync_all_pages()
        
        elif args.list:
            sync.list_cached_pages()
        
        elif args.search:
            results = sync.search_pages(args.search)
            print(f"\nğŸ” '{args.search}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\n")
            for result in results:
                print(f"  - {result['title']}")
                print(f"    {sync.base_url}{result['_links']['webui']}")
        
        else:
            parser.print_help()
    
    except ValueError as e:
        print(f"\nâŒ ì¸ì¦ ì˜¤ë¥˜:\n{e}")
    except requests.exceptions.HTTPError as e:
        print(f"\nâŒ API ì˜¤ë¥˜: {e}")
        if e.response.status_code == 401:
            print("   â†’ API í† í°ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif e.response.status_code == 403:
            print("   â†’ AEGIS ìŠ¤í˜ì´ìŠ¤ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
        elif e.response.status_code == 404:
            print("   â†’ ìŠ¤í˜ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í˜ì´ìŠ¤ í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
